{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIA23YgbXKJd"
      },
      "source": [
        "이를 위해 pyspark과 Py4J 패키지를 설치한다. Py4J 패키지는 파이썬 프로그램이 자바가상머신상의 오브젝트들을 접근할 수 있게 해준다. Local Standalone Spark을 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbT0rpGfVdiq",
        "outputId": "ffa111f6-3aac-4a8f-a768-1ba3a05d48e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark==3.3.1\n",
            "  Using cached pyspark-3.3.1-py2.py3-none-any.whl\n",
            "Collecting py4j==0.10.9.5\n",
            "  Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.4.1\n",
            "    Uninstalling pyspark-3.4.1:\n",
            "      Successfully uninstalled pyspark-3.4.1\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark==3.3.1 py4j==0.10.9.5 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew_eTGrvXlDw"
      },
      "source": [
        "**Spark Session**을 하나 만든다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3vm6tgcPXdnR"
      },
      "outputs": [],
      "source": [
        "# 코드를 작성하기 전에 test 코드를 먼저 만들어두고, 코드를 만들어라\n",
        "# TDD(TEST DRIVEN DEVELOPMENT)\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark Unit Test\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKrMnuGVK77P",
        "outputId": "06de4c0d-4a0a-4f02-e3ed-35580210f7c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n",
            "��ġ ������ �ƴմϴ�.\n"
          ]
        }
      ],
      "source": [
        "!wget https://s3-geospatial.s3-us-west-2.amazonaws.com/name_gender.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZhdS0i7LZEc",
        "outputId": "f0f3fda8-4dcf-4cef-f8db-f8aab3ba6422"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.option(\"header\", True).csv(\"./data/name_gender.csv\")\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGFDiqDcLeq7",
        "outputId": "ebabb5a2-59dd-46c3-f60e-bfc7fe233d1d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmbJXYQ0LlUB",
        "outputId": "913d54cb-4e01-4d3b-d564-056a522899e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-----+\n",
            "|gender|count|\n",
            "+------+-----+\n",
            "|     F|   65|\n",
            "|     M|   28|\n",
            "|Unisex|    7|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.createOrReplaceTempView(\"namegender\")\n",
        "spark.sql(\"SELECT gender, COUNT(1) count FROM namegender GROUP BY 1\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-Y0CL9EpMO-h"
      },
      "outputs": [
        {
          "ename": "Py4JError",
          "evalue": "An error occurred while calling None.org.apache.spark.api.python.PythonFunction. Trace:\npy4j.Py4JException: Constructor org.apache.spark.api.python.PythonFunction([class [B, class java.util.HashMap, class java.util.ArrayList, class java.lang.String, class java.lang.String, class java.util.ArrayList, class org.apache.spark.api.python.PythonAccumulatorV2]) does not exist\r\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:180)\r\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:197)\r\n\tat py4j.Gateway.invoke(Gateway.java:237)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\beginner-spark-programming-with-pyspark\\chapter3\\PySpark_유닛_테스트.ipynb 셀 9\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/beginner-spark-programming-with-pyspark/chapter3/PySpark_%E1%84%8B%E1%85%B2%E1%84%82%E1%85%B5%E1%86%BA_%E1%84%90%E1%85%A6%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m@pandas_udf\u001b[39m(StringType())\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/beginner-spark-programming-with-pyspark/chapter3/PySpark_%E1%84%8B%E1%85%B2%E1%84%82%E1%85%B5%E1%86%BA_%E1%84%90%E1%85%A6%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupper_udf_f\u001b[39m(s: pd\u001b[39m.\u001b[39mSeries) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m pd\u001b[39m.\u001b[39mSeries:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/beginner-spark-programming-with-pyspark/chapter3/PySpark_%E1%84%8B%E1%85%B2%E1%84%82%E1%85%B5%E1%86%BA_%E1%84%90%E1%85%A6%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m s\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mupper()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/beginner-spark-programming-with-pyspark/chapter3/PySpark_%E1%84%8B%E1%85%B2%E1%84%82%E1%85%B5%E1%86%BA_%E1%84%90%E1%85%A6%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m upperUDF \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mudf\u001b[39m.\u001b[39;49mregister(\u001b[39m\"\u001b[39;49m\u001b[39mupper_udf\u001b[39;49m\u001b[39m\"\u001b[39;49m, upper_udf_f)\n",
            "File \u001b[1;32mc:\\Users\\wjddm\\anaconda3\\envs\\spotify\\lib\\site-packages\\pyspark\\sql\\udf.py:456\u001b[0m, in \u001b[0;36mUDFRegistration.register\u001b[1;34m(self, name, f, returnType)\u001b[0m\n\u001b[0;32m    452\u001b[0m     return_udf \u001b[39m=\u001b[39m _create_udf(\n\u001b[0;32m    453\u001b[0m         f, returnType\u001b[39m=\u001b[39mreturnType, evalType\u001b[39m=\u001b[39mPythonEvalType\u001b[39m.\u001b[39mSQL_BATCHED_UDF, name\u001b[39m=\u001b[39mname\n\u001b[0;32m    454\u001b[0m     )\n\u001b[0;32m    455\u001b[0m     register_udf \u001b[39m=\u001b[39m return_udf\u001b[39m.\u001b[39m_unwrapped  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparkSession\u001b[39m.\u001b[39m_jsparkSession\u001b[39m.\u001b[39mudf()\u001b[39m.\u001b[39mregisterPython(name, register_udf\u001b[39m.\u001b[39;49m_judf)\n\u001b[0;32m    457\u001b[0m \u001b[39mreturn\u001b[39;00m return_udf\n",
            "File \u001b[1;32mc:\\Users\\wjddm\\anaconda3\\envs\\spotify\\lib\\site-packages\\pyspark\\sql\\udf.py:215\u001b[0m, in \u001b[0;36mUserDefinedFunction._judf\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_judf\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m JavaObject:\n\u001b[0;32m    210\u001b[0m     \u001b[39m# It is possible that concurrent access, to newly created UDF,\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[39m# will initialize multiple UserDefinedPythonFunctions.\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[39m# This is unlikely, doesn't affect correctness,\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     \u001b[39m# and should have a minimal performance impact.\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_judf_placeholder \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_judf_placeholder \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_judf(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc)\n\u001b[0;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_judf_placeholder\n",
            "File \u001b[1;32mc:\\Users\\wjddm\\anaconda3\\envs\\spotify\\lib\\site-packages\\pyspark\\sql\\udf.py:224\u001b[0m, in \u001b[0;36mUserDefinedFunction._create_judf\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m    221\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39m_getActiveSessionOrCreate()\n\u001b[0;32m    222\u001b[0m sc \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39msparkContext\n\u001b[1;32m--> 224\u001b[0m wrapped_func \u001b[39m=\u001b[39m _wrap_function(sc, func, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreturnType)\n\u001b[0;32m    225\u001b[0m jdt \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39m_jsparkSession\u001b[39m.\u001b[39mparseDataType(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturnType\u001b[39m.\u001b[39mjson())\n\u001b[0;32m    226\u001b[0m \u001b[39massert\u001b[39;00m sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\wjddm\\anaconda3\\envs\\spotify\\lib\\site-packages\\pyspark\\sql\\udf.py:52\u001b[0m, in \u001b[0;36m_wrap_function\u001b[1;34m(sc, func, returnType)\u001b[0m\n\u001b[0;32m     50\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[39m=\u001b[39m _prepare_for_python_RDD(sc, command)\n\u001b[0;32m     51\u001b[0m \u001b[39massert\u001b[39;00m sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m sc\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonFunction(\n\u001b[0;32m     53\u001b[0m     \u001b[39mbytearray\u001b[39;49m(pickled_command),\n\u001b[0;32m     54\u001b[0m     env,\n\u001b[0;32m     55\u001b[0m     includes,\n\u001b[0;32m     56\u001b[0m     sc\u001b[39m.\u001b[39;49mpythonExec,\n\u001b[0;32m     57\u001b[0m     sc\u001b[39m.\u001b[39;49mpythonVer,\n\u001b[0;32m     58\u001b[0m     broadcast_vars,\n\u001b[0;32m     59\u001b[0m     sc\u001b[39m.\u001b[39;49m_javaAccumulator,\n\u001b[0;32m     60\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\wjddm\\anaconda3\\envs\\spotify\\lib\\site-packages\\py4j\\java_gateway.py:1585\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1579\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1580\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_command_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1581\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1582\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1584\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1585\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1586\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gateway_client, \u001b[39mNone\u001b[39;49;00m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fqn)\n\u001b[0;32m   1588\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1589\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
            "File \u001b[1;32mc:\\Users\\wjddm\\anaconda3\\envs\\spotify\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
            "File \u001b[1;32mc:\\Users\\wjddm\\anaconda3\\envs\\spotify\\lib\\site-packages\\py4j\\protocol.py:330\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 330\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n\u001b[0;32m    333\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    334\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    335\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    336\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name))\n",
            "\u001b[1;31mPy4JError\u001b[0m: An error occurred while calling None.org.apache.spark.api.python.PythonFunction. Trace:\npy4j.Py4JException: Constructor org.apache.spark.api.python.PythonFunction([class [B, class java.util.HashMap, class java.util.ArrayList, class java.lang.String, class java.lang.String, class java.util.ArrayList, class org.apache.spark.api.python.PythonAccumulatorV2]) does not exist\r\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:180)\r\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:197)\r\n\tat py4j.Gateway.invoke(Gateway.java:237)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import pandas_udf\n",
        "from pyspark.sql.types import *\n",
        "import pandas as pd\n",
        "\n",
        "# Define the UDF\n",
        "@pandas_udf(StringType())\n",
        "def upper_udf_f(s: pd.Series) -> pd.Series:\n",
        "    return s.str.upper()\n",
        "\n",
        "upperUDF = spark.udf.register(\"upper_udf\", upper_udf_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dRLF06SILaP_"
      },
      "outputs": [],
      "source": [
        "def load_gender(spark, file_path):\n",
        "    return spark.read.option(\"header\", True).csv(file_path)\n",
        "\n",
        "def get_gender_count(spark, df, field_to_count):\n",
        "    df.createOrReplaceTempView(\"namegender_test\")\n",
        "    return spark.sql(f\"SELECT {field_to_count}, COUNT(1) count FROM namegender_test GROUP BY 1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_WT4JmtMsn4",
        "outputId": "1be49c10-784d-4e2f-fb62-a2db56ea5474"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-----+\n",
            "|gender|count|\n",
            "+------+-----+\n",
            "|     F|   65|\n",
            "|     M|   28|\n",
            "|Unisex|    7|\n",
            "+------+-----+\n",
            "\n",
            "+----------+\n",
            "|      NAME|\n",
            "+----------+\n",
            "|  ADALEIGH|\n",
            "|     AMRYN|\n",
            "|    APURVA|\n",
            "|    ARYION|\n",
            "|    ALIXIA|\n",
            "|ALYSSAROSE|\n",
            "|    ARVELL|\n",
            "|     AIBEL|\n",
            "|   ATIYYAH|\n",
            "|     ADLIE|\n",
            "|    ANYELY|\n",
            "|    AAMONI|\n",
            "|     AHMAN|\n",
            "|    ARLANE|\n",
            "|   ARMONEY|\n",
            "|   ATZHIRY|\n",
            "| ANTONETTE|\n",
            "|   AKEELAH|\n",
            "| ABDIKADIR|\n",
            "|    ARINZE|\n",
            "+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = load_gender(spark, \"./data/name_gender.csv\")\n",
        "get_gender_count(spark, df, \"gender\").show()\n",
        "df.select(upperUDF(\"name\").alias(\"NAME\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFg5z8O-Qv54",
        "outputId": "d30f433a-2fc2-4de0-9069-ba1132a1694e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(NAME='ADALEIGH'),\n",
              " Row(NAME='AMRYN'),\n",
              " Row(NAME='APURVA'),\n",
              " Row(NAME='ARYION'),\n",
              " Row(NAME='ALIXIA'),\n",
              " Row(NAME='ALYSSAROSE'),\n",
              " Row(NAME='ARVELL'),\n",
              " Row(NAME='AIBEL'),\n",
              " Row(NAME='ATIYYAH'),\n",
              " Row(NAME='ADLIE'),\n",
              " Row(NAME='ANYELY'),\n",
              " Row(NAME='AAMONI'),\n",
              " Row(NAME='AHMAN'),\n",
              " Row(NAME='ARLANE'),\n",
              " Row(NAME='ARMONEY'),\n",
              " Row(NAME='ATZHIRY'),\n",
              " Row(NAME='ANTONETTE'),\n",
              " Row(NAME='AKEELAH'),\n",
              " Row(NAME='ABDIKADIR'),\n",
              " Row(NAME='ARINZE'),\n",
              " Row(NAME='ARSHAUN'),\n",
              " Row(NAME='ALEXANDRO'),\n",
              " Row(NAME='AYRIAUNA'),\n",
              " Row(NAME='AQIB'),\n",
              " Row(NAME='ALLEYA'),\n",
              " Row(NAME='AAVAH'),\n",
              " Row(NAME='ANESTI'),\n",
              " Row(NAME='ADALAIDE'),\n",
              " Row(NAME='ANALENA'),\n",
              " Row(NAME='ALAEYAH'),\n",
              " Row(NAME='ALBENA'),\n",
              " Row(NAME='AIMI'),\n",
              " Row(NAME='ADWAITH'),\n",
              " Row(NAME='ARKADY'),\n",
              " Row(NAME='ASTYN'),\n",
              " Row(NAME='ADELEE'),\n",
              " Row(NAME='AGATA'),\n",
              " Row(NAME='ALEGNA'),\n",
              " Row(NAME='ALTAN'),\n",
              " Row(NAME='AHNALEIGH'),\n",
              " Row(NAME='ALGIE'),\n",
              " Row(NAME='ASHANTI'),\n",
              " Row(NAME='AISLYN'),\n",
              " Row(NAME='ADALEINE'),\n",
              " Row(NAME='ANTHNOY'),\n",
              " Row(NAME='ALGERNON'),\n",
              " Row(NAME='AERYONA'),\n",
              " Row(NAME='ADRINNE'),\n",
              " Row(NAME='ADDELL'),\n",
              " Row(NAME='AVRIL'),\n",
              " Row(NAME='AHNI'),\n",
              " Row(NAME='AIMON'),\n",
              " Row(NAME='ADOLPHO'),\n",
              " Row(NAME='AHUVA'),\n",
              " Row(NAME='AURIELLE'),\n",
              " Row(NAME='AVEANA'),\n",
              " Row(NAME='ALIYIA'),\n",
              " Row(NAME='ALESANDER'),\n",
              " Row(NAME='ADNREA'),\n",
              " Row(NAME='ANJAE'),\n",
              " Row(NAME='ALVINE'),\n",
              " Row(NAME='ADORAH'),\n",
              " Row(NAME='ADLEMI'),\n",
              " Row(NAME='ALESI'),\n",
              " Row(NAME='ALONTAE'),\n",
              " Row(NAME='ANTONNY'),\n",
              " Row(NAME='ADARAH'),\n",
              " Row(NAME='AYREANNA'),\n",
              " Row(NAME='ANTYON'),\n",
              " Row(NAME='ANDIA'),\n",
              " Row(NAME='ASHLA'),\n",
              " Row(NAME='ASPYN'),\n",
              " Row(NAME='ANTWANETT'),\n",
              " Row(NAME='AUNDREIA'),\n",
              " Row(NAME='AUDELLA'),\n",
              " Row(NAME='AMARI'),\n",
              " Row(NAME='ARSHA'),\n",
              " Row(NAME='ARICELLA'),\n",
              " Row(NAME='ADAN'),\n",
              " Row(NAME='APASRA'),\n",
              " Row(NAME='ALAYSHA'),\n",
              " Row(NAME='ANDERSON'),\n",
              " Row(NAME='AURELIUS'),\n",
              " Row(NAME='AERIAL'),\n",
              " Row(NAME='AVERLEIGH'),\n",
              " Row(NAME='ASLEAN'),\n",
              " Row(NAME='ARNIESHA'),\n",
              " Row(NAME='ASYANA'),\n",
              " Row(NAME='ANNJANE'),\n",
              " Row(NAME='AMABELLA'),\n",
              " Row(NAME='AUSTINJOHN'),\n",
              " Row(NAME='ARLOWEEN'),\n",
              " Row(NAME='ALULA'),\n",
              " Row(NAME='ANEMONE'),\n",
              " Row(NAME='AMORINA'),\n",
              " Row(NAME='ANUREET'),\n",
              " Row(NAME='ARRIC'),\n",
              " Row(NAME='ANTONNE'),\n",
              " Row(NAME='ALYRE'),\n",
              " Row(NAME='ANNAISE')]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.select(upperUDF(\"name\").alias(\"NAME\")).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO6tHzLzLR8G"
      },
      "source": [
        "## 유닛 테스트 코드 붙여보기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeGWFXXpIpOm"
      },
      "outputs": [],
      "source": [
        "from unittest import TestCase\n",
        "\n",
        "# 일반적으로는 아래 함수가 정의된 모듈을 임포트하고 그걸 테스트\n",
        "#  - upper_udf_f\n",
        "#  - load_gender\n",
        "#  - get_gender_count\n",
        "\n",
        "# 보통 테스트 코드를 만든 다음에, 내가 테스트 하고 싶은 함수들을 import 해서 쓰는 것이 일반적\n",
        "\n",
        "# 이 외에도 2가지 방법이 더 존재\n",
        "# - from pyspark.sql.tests import SparkTestingBase\n",
        "# - pytest-spark (pytest testing framework plugin)\n",
        "\n",
        "class UtilsTestCase(TestCase):\n",
        "    # 전역 변수 생성\n",
        "    spark = None\n",
        "\n",
        "    # setUpClass는 계속해서 사용할 리소스들이 있으면, 여기서 생성\n",
        "    # spark 세션을 하나 만들어서 Spark이라는 변수에 저장을 해놓았다.\n",
        "    # self.spark으로 지칭\n",
        "    @classmethod\n",
        "    def setUpClass(cls) -> None:\n",
        "        cls.spark = SparkSession.builder \\\n",
        "            .appName(\"Spark Unit Test\") \\\n",
        "            .getOrCreate()\n",
        "\n",
        "    # 데이터프레임 받아서 그거의 카운트 세서, 100과 같은지 다른지 체크\n",
        "    # assertEqual은 두 개의 인자가 같으면 성공, 실패\n",
        "    def test_datafile_loading(self):\n",
        "        sample_df = load_gender(self.spark, \"./data/name_gender.csv\")\n",
        "        result_count = sample_df.count()\n",
        "        self.assertEqual(result_count, 100, \"Record count should be 100\")\n",
        "\n",
        "    # 데이터 프레임 받은 후\n",
        "    # get gender count해서 콜렉트 해서\n",
        "    # 파이썬에서 row 타입의 리스트로 받아온 후\n",
        "    # 숫자가 맞으면\n",
        "    def test_gender_count(self):\n",
        "        sample_df = load_gender(self.spark, \"./data/name_gender.csv\")\n",
        "        count_list = get_gender_count(self.spark, sample_df, \"gender\").collect()\n",
        "        count_dict = dict()\n",
        "        for row in count_list:\n",
        "            count_dict[row[\"gender\"]] = row[\"count\"]\n",
        "        self.assertEqual(count_dict[\"F\"], 65, \"Count for F should be 65\")\n",
        "        self.assertEqual(count_dict[\"M\"], 28, \"Count for M should be 28\")\n",
        "        self.assertEqual(count_dict[\"Unisex\"], 7, \"Count for Unisex should be 7\")\n",
        "\n",
        "    # udf를 테스트해보는 케이스\n",
        "    # spark 데이터 프레임으로 만들고\n",
        "    # upper_udf 함수 등록\n",
        "    # 대문자로 바뀐 컬럼 이름을 그 데이터프레임의 결과를 collect로 받아온다.\n",
        "    def test_upper_udf(self):\n",
        "        test_data = [\n",
        "            { \"name\": \"John Kim\" },\n",
        "            { \"name\": \"Johnny Kim\"},\n",
        "            { \"name\": \"1234\" }\n",
        "        ]\n",
        "        expected_results = [ \"JOHN KIM\", \"JOHNNY KIM\", \"1234\" ]\n",
        "\n",
        "        upperUDF = self.spark.udf.register(\"upper_udf\", upper_udf_f)\n",
        "        test_df = self.spark.createDataFrame(test_data)\n",
        "        names = test_df.select(\"name\", upperUDF(\"name\").alias(\"NAME\")).collect()\n",
        "        results = []\n",
        "        for name in names:\n",
        "            results.append(name[\"NAME\"])\n",
        "            \n",
        "        # assertCountEqual은 리스트를 두개 받아서 리스트를 sorting 했을 대 동일한지 보는 것\n",
        "        self.assertCountEqual(results, expected_results)\n",
        "\n",
        "    \n",
        "    # tearDownClass는 마지막에 호출\n",
        "    # setUpClass에서 release\n",
        "    @classmethod\n",
        "    def tearDownClass(cls) -> None:\n",
        "        cls.spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFE9zkakGA55",
        "outputId": "7027f30b-4a05-45c0-8a56-dafd61af7297"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "test_datafile_loading (__main__.UtilsTestCase) ... ok\n",
            "test_gender_count (__main__.UtilsTestCase) ... ERROR\n",
            "test_upper_udf (__main__.UtilsTestCase) ... "
          ]
        }
      ],
      "source": [
        "import unittest\n",
        "\n",
        "unittest.main(argv=[''], verbosity=2, exit=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# cli에서 테스트\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "LoCk7SgRrFuP",
        "QkvG7CGo1BgF",
        "YV16sPAT04lt",
        "cdANBnd70u-E",
        "ziIgaC_cXx8S",
        "1bbYGM8MX3zO",
        "9nO5mhnwPozH",
        "uBXdq3jqPwur"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
